{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f163a3d-8861-4711-8162-9e4a296e9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import gradio as gr\n",
    "import time\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Pinecone setup\n",
    "pinecone_api_key = \"\" # Your pinecone api key\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Pinecone index and namespace (same as your previous notebook)\n",
    "index_name = \"\" # pinecone index name\n",
    "namespace = \"\" # pinecone namespace\n",
    "dense_index = pc.Index(index_name)\n",
    "\n",
    "# Huggingface setup\n",
    "HUGGINGFACE_API_KEY = \"\" # your huggingface api token\n",
    "HUGGINGFACE_API_URL = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151247f-27a8-4ba3-bf37-5c96a3da032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Huggingface LLM\n",
    "def query_huggingface(prompt):\n",
    "    headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 256\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(HUGGINGFACE_API_URL, headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        return None\n",
    "\n",
    "# Build LLM prompt\n",
    "def build_llm_prompt(top_chunks, user_question):\n",
    "    chunks_text = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant. Here is information from Victor's resume:\n",
    "\n",
    "=== BEGIN RESUME ===\n",
    "{chunks_text}\n",
    "=== END RESUME ===\n",
    "\n",
    "Now answer the following question concisely:\n",
    "\n",
    "Question: {user_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e87d8-a3b5-4561-bad6-726fe2a6ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(user_question):\n",
    "    # 1. Run Pinecone search\n",
    "    results = dense_index.search(\n",
    "        namespace=namespace,\n",
    "        query={\n",
    "            \"top_k\": 5,\n",
    "            \"inputs\": {\n",
    "                \"text\": user_question\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 2. Extract top 5 chunks\n",
    "    top_chunks = []\n",
    "    for hit in results['result']['hits'][:5]:\n",
    "        chunk_text = hit['fields']['chunk_text']\n",
    "        top_chunks.append(chunk_text)\n",
    "    \n",
    "    # 3. Build LLM prompt\n",
    "    prompt = build_llm_prompt(top_chunks, user_question)\n",
    "    \n",
    "    # 4. Call LLM\n",
    "    response = query_huggingface(prompt)\n",
    "    \n",
    "    # 5. Extract clean answer\n",
    "    generated_text = response[0]['generated_text']\n",
    "    if \"Answer:\" in generated_text:\n",
    "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
    "    else:\n",
    "        answer = generated_text.strip()\n",
    "    \n",
    "    # 6. Return the answer\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83818e5d-1988-4b05-a9f7-efd37f315bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Gradio UI\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(label=\"Ask a question about Victor Jong\", placeholder=\"e.g. What frameworks does Victor know?\"),\n",
    "    outputs=gr.Textbox(label=\"LLM Answer\"),\n",
    "    title=\"Ask Anything about Victor Jong\",\n",
    "    description=\"Ask any question about Victor's resume. Example: 'What frameworks does Victor know?', 'What is Victor's educational background?', 'What programming languages does Victor know?'.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(share=True)  # share=True gives you a public link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a0c62-e977-4fe2-8a15-fb694e454fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
